{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install scipy\n",
    "!pip install torch torchvision \n",
    "!pip install matplotlib\n",
    "# Runtime might need restarting after this installation.\n",
    "!pip install tdqm\n",
    "!pip install twitscrape\n",
    "# twitscrape is a package I created as an aside to this project to further bolster my learning (https://github.com/christywastaken/twitscrape, https://pypi.org/project/twitscrape). \n",
    "# I'm sure there are more effective ways of collecting the tweet data used in this project. If you are looking to repeat this data analysis I would reccomend looking for another package such as snscrape.\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_tweets(tweet_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cleaned_tweet_df = tweet_df.copy()\n",
    "    for index, row in cleaned_tweet_df.iterrows():\n",
    "        tweet_text = row['tweet_text']\n",
    "        clean_tweet_text = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet_text)\n",
    "        #TODO: Should I replace @mentions with a name like 'Daniel' to help the sentiment analysis? Or should I include @mentions as original? test this.\n",
    "        clean_tweet_text = re.sub(\"#[A-Za-z0-9_]+\",\"\", clean_tweet_text)\n",
    "        clean_tweet_text = clean_tweet_text.lower()\n",
    "        print(f'\\ntweet: {tweet_text}\\nclean: {clean_tweet_text}\\n')\n",
    "        \n",
    "        cleaned_tweet_df.at[index, 'cleaned_tweet_text'] = clean_tweet_text\n",
    "    \n",
    "    return cleaned_tweet_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class SQLContextManager():\n",
    "    \"\"\" Context manager for SQL db \"\"\"\n",
    "    def __init__(self, host):\n",
    "        self.connection = None\n",
    "        self.host = host\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.connection = sqlite3.connect(self.host)\n",
    "        return self.connection\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_val, exc_tab):\n",
    "        self.connection.commit()\n",
    "        self.connection.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Check licensing and citing for https://github.com/cardiffnlp/timelms \n",
    "#TODO: consider saving model to local.\n",
    "\n",
    "class SentimentAnalysis():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL)\n",
    "        self.config = AutoConfig.from_pretrained(self.MODEL)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.MODEL)\n",
    "\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "        return \" \".join(new_text)\n",
    "\n",
    "\n",
    "    def analyse_sentiment(self, tweet_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" Analyses the sentiment of tweets and returns a dataframe with new columns 'negative', 'neutral' and 'positive' \"\"\"\n",
    "        tweet_df_copy = tweet_df.copy()\n",
    "        # model.save_pretrained(MODEL) #TODO: consider saving to local\n",
    "        for index, row in tqdm(tweet_df_copy.iterrows()):\n",
    "            try:\n",
    "                tweet_text = row['tweet_text']\n",
    "                tweet_text = self.preprocess(tweet_text)\n",
    "                encoded_input = self.tokenizer(tweet_text, return_tensors='pt')\n",
    "                output = self.model(**encoded_input)\n",
    "                scores = output[0][0].detach().numpy()\n",
    "                scores = softmax(scores)\n",
    "                scores = np.round(scores.astype(float), 4)\n",
    "                sentiment_dict = {self.config.id2label[i]: score for i, score in enumerate(scores)}\n",
    "                for column, value in sentiment_dict.items():\n",
    "                    tweet_df_copy.at[index, column] = value\n",
    "                # print(f'tweet: {tweet_text} | senti: {sentiment_dict}')\n",
    "            except Exception as err:\n",
    "                print(f'Error at index:{index} | Error: {err}')\n",
    "        return tweet_df_copy\n",
    "\n",
    "\n",
    "    def analyse_sentiment_row(self, tweet_text: str) -> list[float, float, float]:\n",
    "        try:\n",
    "            tweet_text = self.preprocess(tweet_text)\n",
    "            encoded_input = self.tokenizer(tweet_text, return_tensors='pt')\n",
    "            output = self.model(**encoded_input)\n",
    "            scores = output[0][0].detach().numpy()\n",
    "            scores = softmax(scores)\n",
    "            scores = np.round(scores.astype(float), 4)\n",
    "        except Exception as err:\n",
    "            print(f'Error: {err}')\n",
    "            return [None, None, None]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Analysing sentiment of tweet_text column: 100%|██████████| 958/958 [02:22<00:00,  6.73it/s]\n"
     ]
    }
   ],
   "source": [
    "with SQLContextManager('twitter_sentiment_analysis.db') as connection:\n",
    "    query = \"SELECT * FROM summer_solstice_week\"\n",
    "    test_tweet_df = pd.read_sql(query, connection)\n",
    "    analyse_sentiment = SentimentAnalysis().analyse_sentiment_row\n",
    "    tqdm.pandas(desc=\"Analysing sentiment of tweet_text column\")\n",
    "    test_tweet_df['negative', 'neutral', 'positive'] = test_tweet_df['tweet_text'].progress_apply(analyse_sentiment)\n",
    "    test_tweet_df.to_sql('test_table_apply_method', con=connection, if_exists='replace')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "958it [02:21,  6.77it/s]\n"
     ]
    }
   ],
   "source": [
    "with SQLContextManager('twitter_sentiment_analysis.db') as connection:\n",
    "    query = \"SELECT * FROM summer_solstice_week\"\n",
    "    test_tweet_df = pd.read_sql(query, connection)\n",
    "    analyse_sentiment = SentimentAnalysis().analyse_sentiment\n",
    "    test_tweet_df = analyse_sentiment(test_tweet_df)\n",
    "    test_tweet_df.to_sql('test_table_apply_method', con=connection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def add_epoch_time_and_sort(tweet_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Adds epoch_seconds column to df and sorts by epoch_seconds ascending \"\"\"\n",
    "    \n",
    "    tweet_df_copy = tweet_df.copy()\n",
    "    date_format = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "\n",
    "    for index, row in tweet_df_copy.iterrows():\n",
    "        try:\n",
    "            created_at = row['created_at']\n",
    "            epoch_seconds = datetime.strptime(created_at, date_format).timestamp()\n",
    "            tweet_df_copy.at[index, 'epoch_seconds'] = epoch_seconds\n",
    "        except Exception as err:\n",
    "            print(f'Error: {err}')\n",
    "    tweet_df_copy.sort_values(by='epoch_seconds', inplace=True, ignore_index=True)\n",
    "    tweet_df_copy.reset_index(inplace=True, drop=True)\n",
    "    return tweet_df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether data is missing\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def check_gaps_in_data(tweet_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Adds time_diff_mins coloumn to df which can be used for identifying holes in data. This column is the time betweent the current and next tweet \"\"\"\n",
    "    tweet_df_copy = tweet_df.copy()\n",
    "    \n",
    "    largest_time_diff = 0.0\n",
    "    index_largest_diff = 0\n",
    "    for index, row in tweet_df_copy.iterrows():\n",
    "        try:\n",
    "            epoch_seconds1 = tweet_df_copy.iloc[index]['epoch_seconds']\n",
    "            epoch_seconds2 = tweet_df_copy.iloc[index+1]['epoch_seconds']\n",
    "            \n",
    "            time_diff = epoch_seconds2 - epoch_seconds1\n",
    "            time_diff_mins = round((time_diff / 60), 2)\n",
    "            if time_diff_mins > largest_time_diff:\n",
    "                largest_time_diff = time_diff_mins\n",
    "                index_largest_diff = index\n",
    "            tweet_df_copy.at[index, 'time_diff_mins'] = time_diff_mins\n",
    "            \n",
    "\n",
    "        except Exception as err:\n",
    "            print(f'Error: {err}')\n",
    "    print(f'-- The largest gap in data is: {largest_time_diff} mins at index: {index_largest_diff} --')\n",
    "    return tweet_df_copy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from twitscrape.twitter_scraper import TwitterGeolocationScraper \n",
    "# twitscrape is a package I built and distributed as an aside learning experiment to this project. I plan to work on it further so the import method might change from the above.\n",
    "# https://github.com/christywastaken/twitscrape \n",
    "# https://pypi.org/project/twitscrape/ \n",
    "\n",
    "newcastle_latitude = 54.975029\n",
    "newcastle_longitude = -1.612477\n",
    "search_radius_km = 15.0\n",
    "twitter_scraper = TwitterGeolocationScraper(start_date='2022-01-01', end_date='2023-01-01', latitude=newcastle_latitude, longitude=newcastle_longitude, radius=search_radius_km, filter_links=True, filter_replies=True, is_headless=True)\n",
    "tweet_df = twitter_scraper.run()\n",
    "# Creates dataframe of all the tweets in Newcastle-Upon-Tyne for a 15km radius for all of 2022. No replies or tweets containing media included.\n",
    "\n",
    "#Store DF to SQL DB\n",
    "with SQLContextManager('twitter_sentiment_analysis.db') as connection:\n",
    "    tweet_df.to_sql('all_2022', connection, if_exists='replace', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add epoch_seconds to SQL db.\n",
    "with SQLContextManager('twitter_sentiment_analysis.db') as connection:\n",
    "    query = \"SELECT * FROM all_2022\"\n",
    "    tweet_df = pd.read_sql(query, connection)\n",
    "    epoc_tweet_df = add_epoch_time_and_sort(tweet_df=tweet_df)\n",
    "    epoc_tweet_df.to_sql('all_2022', con=connection, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: single positional indexer is out-of-bounds\n",
      "-- The largest gap in data is: 556.58 mins at index: 30882 --\n"
     ]
    }
   ],
   "source": [
    "# check for gaps in data and sort by largest gaps.\n",
    "with SQLContextManager('twitter_sentiment_analysis.db') as connection:\n",
    "    query = \"SELECT * FROM all_2022\"\n",
    "    tweet_df = pd.read_sql(query, connection)\n",
    "\n",
    "tweet_df_time_gaps = check_gaps_in_data(tweet_df)\n",
    "tweet_df_time_gaps.sort_values(by='time_diff_mins', inplace=True, ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visiualizing gaps in the data \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(tweet_df_time_gaps['time_diff_mins'], bins='auto', edgecolor='black')\n",
    "plt.title('Histogram of Time Gaps Between Tweets')\n",
    "plt.xlabel('Time Gap (minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of tweets with gap >60 mins: 2.2118%\n",
      "Percentage of tweets with gap >120 mins: 0.7333%\n",
      "Percentage of tweets with gap >360 mins: 0.0198%\n",
      "Percentage of tweets with gap >420 mins: 0.004%\n",
      "Percentage of tweets with gap >480 mins: 0.002%\n"
     ]
    }
   ],
   "source": [
    "# Getting some numbers on the gaps in the data\n",
    "data_size = len(tweet_df_time_gaps)\n",
    "for time in [60,120,360,420,480]:\n",
    "    num_gaps = sum(tweet_df_time_gaps['time_diff_mins']>time)\n",
    "    print(f'Percentage of tweets with gap >{str(time)} mins: {round(((num_gaps/data_size)*100),4)}%')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "653it [01:26,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The expanded size of the tensor (535) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 535].  Tensor sizes: [1, 514]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1316it [03:00,  7.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# analyse the sentiment of the data. \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m analyse_sentiment_and_update_sql_table(table_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mall_2022\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36manalyse_sentiment_and_update_sql_table\u001b[0;34m(table_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSELECT * FROM \u001b[39m\u001b[39m{\u001b[39;00mtable_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m tweet_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_sql(query, connection)\n\u001b[0;32m----> 8\u001b[0m sentiment_tweet_df \u001b[39m=\u001b[39m analyse_sentiment(tweet_df\u001b[39m=\u001b[39;49mtweet_df)\n\u001b[1;32m      9\u001b[0m sentiment_tweet_df\u001b[39m.\u001b[39mto_sql(table_name, con\u001b[39m=\u001b[39mconnection, if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[49], line 37\u001b[0m, in \u001b[0;36manalyse_sentiment\u001b[0;34m(tweet_df)\u001b[0m\n\u001b[1;32m     35\u001b[0m tweet_text \u001b[39m=\u001b[39m preprocess(tweet_text)\n\u001b[1;32m     36\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(tweet_text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded_input)\n\u001b[1;32m     38\u001b[0m scores \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     39\u001b[0m scores \u001b[39m=\u001b[39m softmax(scores)\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1212\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1212\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1213\u001b[0m     input_ids,\n\u001b[1;32m   1214\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1215\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1216\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1217\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1218\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1219\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1220\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1221\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1222\u001b[0m )\n\u001b[1;32m   1223\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1224\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    853\u001b[0m     embedding_output,\n\u001b[1;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    855\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    856\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    857\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    858\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    863\u001b[0m )\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    520\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    528\u001b[0m         hidden_states,\n\u001b[1;32m    529\u001b[0m         attention_mask,\n\u001b[1;32m    530\u001b[0m         layer_head_mask,\n\u001b[1;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    533\u001b[0m         past_key_value,\n\u001b[1;32m    534\u001b[0m         output_attentions,\n\u001b[1;32m    535\u001b[0m     )\n\u001b[1;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    450\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    451\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 453\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    455\u001b[0m )\n\u001b[1;32m    456\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    458\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:465\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> 465\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    466\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:363\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 363\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    364\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    365\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/learn/purple-beard/capstone/sentiment_analysis/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# analyse the sentiment of the data. \n",
    "with SQLContextManager('twitter_sentiment_analysis.db') as connection:\n",
    "    query = \"SELECT * FROM all_2022\"\n",
    "    tweet_df = pd.read_sql(query, connection)\n",
    "\n",
    "sentiment_tweet_df = analyse_sentiment(tweet_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9128e2c337b576c98b0bbaa62dd69fcada41be6d3adb1a650c84481eadacf543"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
